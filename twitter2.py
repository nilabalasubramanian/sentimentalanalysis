# -*- coding: utf-8 -*-
"""Twitter2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1bxoQl1bd2kCAAfSkSbyOHV8z3WbCvqiG
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')
pd.set_option('display.max_columns',None)

from wordcloud import WordCloud,STOPWORDS
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
from xgboost import XGBClassifier
from sklearn.preprocessing import LabelEncoder

import re
import nltk
from nltk import word_tokenize
nltk.download('stopwords')

train = pd.read_csv("twitter_training.csv",header=None)
val = pd.read_csv("twitter_validation.csv",header=None)
train.columns = ['id','information','sentiment','text']
val.columns = ['id','information','sentiment','text']
train.head()

val.head()

#dataframe1 = train[train['sentiment']=='Positive']
df1 = train[train['sentiment'] == 'Positive']
words = ''.join(df1['text'].astype(str))
cleaned_word = ''.join([word for word in words.split() if not word.startswith('@')])
#wordcloud = WordCloud(background_color='black',stopword=STOPWORDS,width=3000,height=2500).generate(''.join(cleaned_word))
wordcloud = WordCloud(background_color='black',stopwords=STOPWORDS,
                      width=3000, height=2500).generate(''.join(cleaned_word))
plt.imshow(wordcloud)
plt.axis("off")
plt.show()

df2 = train[train['sentiment']=='Negative']
words = ''.join(df2['text'].astype(str))
cleaned_word = ''.join([word for word in words.split() if not words.startswith('@')])
wordcloud = WordCloud(background_color='black',stopwords=STOPWORDS,width=3000,height=2500).generate(''.join(cleaned_word))
plt.imshow(wordcloud)
plt.axis("off")
plt.show()

df3 = train[train['sentiment']=='Irrelevant']
words = ''.join(df3['text'].astype(str))
cleaned_word = ''.join([word for word in words.split() if not words.startswith('@')])
wordcloud = WordCloud(background_color='black',stopwords='STOPWORDS',width=3000,height=2500).generate(''.join(cleaned_word))
plt.imshow(wordcloud)
plt.axis("off")
plt.show()

df4 = train[train['sentiment']=='Neutral']
words = ''.join(df4['text'].astype(str))
cleaned_word = ''.join([word for word in words.split() if not words.startswith('@')])
wordcloud = WordCloud(background_color='black',stopwords=STOPWORDS,height=2500,width=3000).generate(''.join(cleaned_word))
plt.imshow(wordcloud)
plt.axis("off")
plt.show()

plot1 = train.groupby(by=['information','sentiment']).count().reset_index()
plot1.head()

plt.figure(figsize=(20,6))
sns.barplot(data=plot1,x='information',y='id',hue='sentiment')
plt.xticks(rotation=90)
plt.xlabel("Brand")
plt.ylabel("Number of Tweets")
plt.grid()
plt.title("Distribution of Tweets per Brand and Sentiment")

import nltk
import re
from nltk.corpus import stopwords

def tweet_to_words(tweet):
    letters_only = re.sub('^[a-zA-Z]',' ',tweet)
    words = letters_only.lower().split()
    stops = set(stopwords.words('english'))
    meaningful_word = [w for w in words if w not in stops]
    return ' '.join(meaningful_word)

def tweet_len(tweet):
    letters_only = re.sub('^[a-zA-Z]',' ',tweet)
    words = letters_only.lower().split()
    stops = set(stopwords.words('english'))
    meaningful_word = [w for w in words if w not in stops]
    return (len(meaningful_word))

train['sentiments'] = train['sentiment'].apply(lambda x:0 if x=='negative' else 1)
train['clean_tweet'] = train['text'].astype(str).apply(lambda x:tweet_to_words(x))
train['tweet_length'] = train['text'].astype(str).apply(lambda x:tweet_len(x))
train_data, test_data = train_test_split(train,test_size=0.2,random_state=42)

train_clean_tweet=[]
for tweet in train_data['clean_tweet']:
   train_clean_tweet.append(tweet)
test_clean_tweet=[]
for tweet in test_data['clean_tweet']:
    test_clean_tweet.append(tweet)

from sklearn.feature_extraction.text import CountVectorizer
v = CountVectorizer(analyzer='word')
train_features = v.fit_transform(train_clean_tweet)
test_features = v.transform(test_clean_tweet)

from sklearn.linear_model import LinearRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC,LinearSVC,NuSVC
from sklearn.metrics import accuracy_score
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier,GradientBoostingClassifier
from sklearn.naive_bayes import GaussianNB

Classifiers=[
    LogisticRegression(C=0.000000001,solver='liblinear',max_iter=200),
    KNeighborsClassifier(3),
    DecisionTreeClassifier(),
    RandomForestClassifier(n_estimators=200),
    AdaBoostClassifier(),
    GaussianNB()
]

dense_features = train_features.toarray()
densetest = test_features.toarray()
Accuracy=[]
Model=[]
for classifier in Classifiers:
    try:
       fit =  classifier.fit(train_features,train_data['sentiment'])
       pred = fit.predict(test_features)
    except Exception:
       fit = classifier.fit(dense_features,train_data['sentiment'])
       pred = fit.predict(dense_test)
    accuracy = accuracy_score(pred,test_data['sentiment'])
    Accuracy.append(accuracy)
    Model.append(classifier.__class__.__name__)
    print('Accuracy of'+classifier.__class__.__name__+'is'+str(accuracy))

result = pd.DataFrame({'Models':Model})
result['Accuracy'] = Accuracy
result = result.sort_values(by='Accuracy',ascending=False)
result